"""
Code Review Pipeline

A composite workflow that combines CodeReviewCrew with CodeReviewWorkflow
for comprehensive code analysis.

Modes:
- full: Run CodeReviewCrew (5 agents) + CodeReviewWorkflow
- standard: Run CodeReviewWorkflow only
- quick: Run classify + scan stages only (skip architect review)

Copyright 2025 Smart-AI-Memory
Licensed under Fair Source License 0.9
"""

import asyncio
import logging
import time
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)


@dataclass
class CodeReviewPipelineResult:
    """Result from CodeReviewPipeline execution."""

    success: bool
    verdict: str  # "approve", "approve_with_suggestions", "request_changes", "reject"
    quality_score: float
    crew_report: dict | None
    workflow_result: dict | None
    combined_findings: list[dict]
    critical_count: int
    high_count: int
    medium_count: int
    agents_used: list[str]
    recommendations: list[str]
    blockers: list[str]
    mode: str
    duration_seconds: float
    cost: float
    metadata: dict = field(default_factory=dict)


class CodeReviewPipeline:
    """
    Composite workflow combining CodeReviewCrew with CodeReviewWorkflow.

    Provides multiple modes for different use cases:
    - full: Most comprehensive (crew + workflow)
    - standard: Balanced (workflow only)
    - quick: Fast check (minimal stages)

    Usage:
        pipeline = CodeReviewPipeline(mode="full")
        result = await pipeline.execute(
            diff="...",
            files_changed=["src/main.py"],
        )

        # Or use factory methods:
        pipeline = CodeReviewPipeline.for_pr_review(files_changed=12)
        pipeline = CodeReviewPipeline.for_quick_check()
    """

    def __init__(
        self,
        provider: str = "anthropic",
        mode: str = "full",
        parallel_crew: bool = True,
        crew_config: dict | None = None,
        **kwargs,
    ):
        """
        Initialize the pipeline.

        Args:
            provider: LLM provider to use (anthropic, openai, etc.)
            mode: Review mode ("full", "standard", "quick")
            parallel_crew: Run crew in parallel with workflow (full mode only)
            crew_config: Configuration for CodeReviewCrew
            **kwargs: Additional arguments (for CLI compatibility)
        """
        self.provider = provider
        self.mode = mode
        self.parallel_crew = parallel_crew
        # Inject provider into crew config
        self.crew_config = {"provider": provider, **(crew_config or {})}
        self.crew_enabled = mode == "full"

    @classmethod
    def for_pr_review(cls, files_changed: int = 0) -> "CodeReviewPipeline":
        """
        Factory for PR review - uses crew for complex PRs.

        Args:
            files_changed: Number of files changed in PR

        Returns:
            Pipeline configured for PR review complexity
        """
        # Use full mode for complex PRs (5+ files)
        mode = "full" if files_changed > 5 else "standard"
        return cls(mode=mode, parallel_crew=True)

    @classmethod
    def for_quick_check(cls) -> "CodeReviewPipeline":
        """Quick code check without crew - minimal analysis."""
        return cls(mode="quick", parallel_crew=False)

    @classmethod
    def for_full_review(cls) -> "CodeReviewPipeline":
        """Full review with all agents and workflow stages."""
        return cls(mode="full", parallel_crew=True)

    async def execute(
        self,
        diff: str = "",
        files_changed: list[str] | None = None,
        target: str = "",
        context: dict | None = None,
    ) -> CodeReviewPipelineResult:
        """
        Execute the code review pipeline.

        Args:
            diff: Code diff to review
            files_changed: List of changed files
            target: Target file/folder path (alternative to diff)
            context: Additional context for review

        Returns:
            CodeReviewPipelineResult with combined analysis
        """
        start_time = time.time()
        files_changed = files_changed or []
        context = context or {}

        # Initialize result collectors
        crew_report: dict | None = None
        workflow_result: dict | None = None
        all_findings: list[dict] = []
        recommendations: list[str] = []
        blockers: list[str] = []
        agents_used: list[str] = []
        total_cost = 0.0

        # Get code to review
        code_to_review = diff or target

        try:
            if self.mode == "full":
                # Run crew and workflow
                crew_report, workflow_result = await self._run_full_mode(
                    code_to_review, files_changed, context
                )
            elif self.mode == "standard":
                # Run workflow only
                workflow_result = await self._run_standard_mode(
                    code_to_review, files_changed, context
                )
            else:  # quick
                # Run minimal workflow
                workflow_result = await self._run_quick_mode(code_to_review, files_changed, context)

            # Aggregate findings
            if crew_report:
                crew_findings = crew_report.get("findings", [])
                all_findings.extend(crew_findings)
                agents_used = crew_report.get("agents_used", [])

                # Extract crew recommendations
                for finding in crew_findings:
                    if finding.get("suggestion"):
                        recommendations.append(finding["suggestion"])

            if workflow_result:
                # Get workflow findings from various stages
                wf_output = workflow_result.get("final_output", {})
                scan_findings = wf_output.get("security_findings", [])
                all_findings.extend(scan_findings)

                # Get cost from workflow
                cost_report = workflow_result.get("cost_report", {})
                if hasattr(cost_report, "total_cost"):
                    total_cost = cost_report.total_cost

            # Deduplicate findings by (file, line, type)
            all_findings = self._deduplicate_findings(all_findings)

            # Count by severity
            critical_count = len([f for f in all_findings if f.get("severity") == "critical"])
            high_count = len([f for f in all_findings if f.get("severity") == "high"])
            medium_count = len([f for f in all_findings if f.get("severity") == "medium"])

            # Determine blockers
            if critical_count > 0:
                blockers.append(f"{critical_count} critical issue(s) found")
            if high_count > 3:
                blockers.append(f"{high_count} high severity issues (threshold: 3)")

            # Calculate combined scores
            quality_score = self._calculate_quality_score(
                crew_report, workflow_result, all_findings
            )

            # Determine verdict
            verdict = self._determine_verdict(crew_report, workflow_result, quality_score, blockers)

            duration = time.time() - start_time

            return CodeReviewPipelineResult(
                success=True,
                verdict=verdict,
                quality_score=quality_score,
                crew_report=crew_report,
                workflow_result=workflow_result,
                combined_findings=all_findings,
                critical_count=critical_count,
                high_count=high_count,
                medium_count=medium_count,
                agents_used=agents_used,
                recommendations=recommendations[:10],  # Top 10
                blockers=blockers,
                mode=self.mode,
                duration_seconds=duration,
                cost=total_cost,
                metadata={
                    "files_reviewed": len(files_changed),
                    "total_findings": len(all_findings),
                    "crew_enabled": self.crew_enabled,
                    "parallel_crew": self.parallel_crew,
                },
            )

        except Exception as e:
            logger.error(f"CodeReviewPipeline failed: {e}")
            duration = time.time() - start_time
            return CodeReviewPipelineResult(
                success=False,
                verdict="reject",
                quality_score=0.0,
                crew_report=crew_report,
                workflow_result=workflow_result,
                combined_findings=all_findings,
                critical_count=0,
                high_count=0,
                medium_count=0,
                agents_used=agents_used,
                recommendations=[],
                blockers=[f"Pipeline error: {str(e)}"],
                mode=self.mode,
                duration_seconds=duration,
                cost=total_cost,
                metadata={"error": str(e)},
            )

    async def _run_full_mode(
        self,
        code_to_review: str,
        files_changed: list[str],
        context: dict,
    ) -> tuple[dict | None, dict | None]:
        """Run full mode with crew and workflow."""
        from .code_review import CodeReviewWorkflow
        from .code_review_adapters import (
            _check_crew_available,
            _get_crew_review,
            crew_report_to_workflow_format,
        )

        crew_report: dict | None = None
        workflow_result: dict | None = None

        # Check if crew is available
        crew_available = _check_crew_available()

        if crew_available and self.parallel_crew:
            # Run crew and workflow in parallel
            crew_task = asyncio.create_task(
                _get_crew_review(
                    diff=code_to_review,
                    files_changed=files_changed,
                    config=self.crew_config,
                )
            )

            # Run workflow (without crew - we'll merge results)
            workflow = CodeReviewWorkflow(use_crew=False)
            workflow_task = asyncio.create_task(
                workflow.execute(
                    diff=code_to_review,
                    files_changed=files_changed,
                    **context,
                )
            )

            # Wait for both
            crew_report_obj, workflow_result = await asyncio.gather(
                crew_task, workflow_task, return_exceptions=True
            )

            # Handle crew result
            if isinstance(crew_report_obj, Exception):
                logger.warning(f"Crew review failed: {crew_report_obj}")
            elif crew_report_obj:
                crew_report = crew_report_to_workflow_format(crew_report_obj)

            # Handle workflow result
            if isinstance(workflow_result, Exception):
                logger.warning(f"Workflow failed: {workflow_result}")
                workflow_result = None

        elif crew_available:
            # Run sequentially
            crew_report_obj = await _get_crew_review(
                diff=code_to_review,
                files_changed=files_changed,
                config=self.crew_config,
            )
            if crew_report_obj:
                crew_report = crew_report_to_workflow_format(crew_report_obj)

            workflow = CodeReviewWorkflow(use_crew=False)
            workflow_result = await workflow.execute(
                diff=code_to_review,
                files_changed=files_changed,
                **context,
            )
        else:
            # Crew not available, run workflow only
            logger.info("CodeReviewCrew not available, running workflow only")
            workflow = CodeReviewWorkflow(use_crew=False)
            workflow_result = await workflow.execute(
                diff=code_to_review,
                files_changed=files_changed,
                **context,
            )

        return crew_report, workflow_result

    async def _run_standard_mode(
        self,
        code_to_review: str,
        files_changed: list[str],
        context: dict,
    ) -> dict:
        """Run standard mode with workflow only."""
        from .code_review import CodeReviewWorkflow

        workflow = CodeReviewWorkflow(use_crew=False)
        result = await workflow.execute(
            diff=code_to_review,
            files_changed=files_changed,
            **context,
        )
        return result

    async def _run_quick_mode(
        self,
        code_to_review: str,
        files_changed: list[str],
        context: dict,
    ) -> dict:
        """Run quick mode with minimal stages."""
        from .code_review import CodeReviewWorkflow

        # Use workflow but it will skip architect_review for simple changes
        workflow = CodeReviewWorkflow(
            file_threshold=1000,  # High threshold = skip architect review
            use_crew=False,
        )
        result = await workflow.execute(
            diff=code_to_review,
            files_changed=files_changed,
            is_core_module=False,
            **context,
        )
        return result

    def _deduplicate_findings(self, findings: list[dict]) -> list[dict]:
        """Deduplicate findings by (file, line, type)."""
        seen = set()
        unique = []
        for f in findings:
            key = (f.get("file"), f.get("line"), f.get("type"))
            if key not in seen:
                seen.add(key)
                unique.append(f)
        return unique

    def _calculate_quality_score(
        self,
        crew_report: dict | None,
        workflow_result: dict | None,
        findings: list[dict],
    ) -> float:
        """Calculate combined quality score."""
        scores = []
        weights = []

        # Crew quality score (if available)
        if crew_report:
            crew_score = crew_report.get("quality_score", 100)
            scores.append(crew_score)
            weights.append(1.5)  # Crew gets higher weight

        # Workflow security score (if available)
        if workflow_result:
            wf_output = workflow_result.get("final_output", {})
            security_score = wf_output.get("security_score", 90)
            scores.append(security_score)
            weights.append(1.0)

        # Calculate weighted average
        if scores:
            weighted_sum = sum(s * w for s, w in zip(scores, weights, strict=False))
            quality_score = weighted_sum / sum(weights)
        else:
            # Fallback: deduct based on findings
            quality_score = 100.0
            for f in findings:
                sev = f.get("severity", "medium")
                if sev == "critical":
                    quality_score -= 25
                elif sev == "high":
                    quality_score -= 15
                elif sev == "medium":
                    quality_score -= 5
                elif sev == "low":
                    quality_score -= 2

        return max(0.0, min(100.0, quality_score))

    def _determine_verdict(
        self,
        crew_report: dict | None,
        workflow_result: dict | None,
        quality_score: float,
        blockers: list[str],
    ) -> str:
        """Determine final verdict based on all inputs."""
        # Start with most severe verdict
        verdict_priority = ["reject", "request_changes", "approve_with_suggestions", "approve"]

        verdicts = []

        # Crew verdict
        if crew_report:
            crew_verdict = crew_report.get("verdict", "approve")
            verdicts.append(crew_verdict)

        # Workflow verdict (from architect review)
        if workflow_result:
            wf_output = workflow_result.get("final_output", {})
            wf_verdict = wf_output.get("verdict", "approve")
            verdicts.append(wf_verdict)

        # Score-based verdict
        if quality_score < 50:
            verdicts.append("reject")
        elif quality_score < 70:
            verdicts.append("request_changes")
        elif quality_score < 90:
            verdicts.append("approve_with_suggestions")
        else:
            verdicts.append("approve")

        # Blocker-based verdict
        if blockers:
            verdicts.append("request_changes")

        # Take most severe
        for v in verdict_priority:
            if v in verdicts:
                return v

        return "approve"


# CLI entry point
def main():
    """Run CodeReviewPipeline from command line."""
    import argparse

    parser = argparse.ArgumentParser(description="Code Review Pipeline")
    parser.add_argument("--diff", "-d", help="Code diff to review")
    parser.add_argument("--file", "-f", help="File to review")
    parser.add_argument(
        "--mode",
        "-m",
        default="full",
        choices=["full", "standard", "quick"],
        help="Review mode",
    )
    parser.add_argument(
        "--parallel/--sequential",
        dest="parallel",
        default=True,
        help="Run crew in parallel",
    )

    args = parser.parse_args()

    async def run():
        pipeline = CodeReviewPipeline(mode=args.mode, parallel_crew=args.parallel)

        diff = args.diff or ""
        if args.file:
            try:
                with open(args.file) as f:
                    diff = f.read()
            except FileNotFoundError:
                print(f"File not found: {args.file}")
                return

        result = await pipeline.execute(diff=diff)

        print("\n" + "=" * 60)
        print("CODE REVIEW PIPELINE RESULTS")
        print("=" * 60)
        print(f"Mode: {result.mode}")
        print(f"Verdict: {result.verdict.upper()}")
        print(f"Quality Score: {result.quality_score:.1f}/100")
        print(f"Duration: {result.duration_seconds:.2f}s")
        print(f"Cost: ${result.cost:.4f}")

        if result.agents_used:
            print(f"\nAgents Used: {', '.join(result.agents_used)}")

        print(f"\nFindings: {len(result.combined_findings)} total")
        print(f"  Critical: {result.critical_count}")
        print(f"  High: {result.high_count}")
        print(f"  Medium: {result.medium_count}")

        if result.blockers:
            print("\nBlockers:")
            for b in result.blockers:
                print(f"  - {b}")

        if result.recommendations[:5]:
            print("\nTop Recommendations:")
            for r in result.recommendations[:5]:
                print(f"  - {r[:100]}...")

    asyncio.run(run())


if __name__ == "__main__":
    main()
